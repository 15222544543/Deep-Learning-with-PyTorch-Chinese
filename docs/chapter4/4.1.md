# 4.1 学习就是参数估计

在本节，你将学习如何获取数据，选择模型并估计模型的参数，以便对新数据给出良好的预测。为此，您将摆脱行星运动的复杂性，将注意力转移到物理学上第二困难的问题上：仪器校准。

图4.2简要概述了本章结束时你将要实现的内容。给定输入数据和相应的期望输出（ground truth）以及权重的初始值，模型输入数据（前向传播），然后通过把结果输出与ground truth进行比较来评估误差。为了优化模型的参数，其权重（即单位权重变化引起的误差变化，也即误差相对于参数的梯度）通过使用对复合函数求导的链式法则进行计算（反向传播）。然后，权重的值沿导致误差减小的方向更新。不断重复该过程直到在新数据上的评估误差降至可接受的水平以下。

上述内容你听起来可能有点晦涩难懂，我们将用整整一章进行阐释。待我们完成时，所有内容都将就位，到时你就能理解上一段的意义。

接下来，你要处理含噪声的数据集，建立模型并为其实现一个学习算法。你首先手动完成所有操作，但是到本章结束时，你就可以让PyTorch完成所有繁重的工作。到本章结束时，我们将涵盖训练深度神经网络的许多基本概念，即使示例很简单且其模型（还）不是神经网络。

<div align=center>
<img width="600" src="../img/chapter4/4.2.png" alt="4.2"/>
</div>
<div align=center>图4.2 模型的学习过程</div>

## 4.1.1 一个热门的问题

假设你去了一些鲜为人知的地方旅游，然后带回了一个花哨的壁挂式模拟温度计。这个温度计看起来很棒，非常适合你的客厅。唯一的缺点是它不显示单位。不用担心，你有一个计划。你用自己喜欢的单位建立一个读数和相应温度值的数据集，然后选择一个模型，并迭代调整单位的权重，直到误差的测量值足够低为止，最后你就可以在新温度计上进行准确读数了。

首先记录能正常工作的旧摄氏温度计的数据和你刚带回来的新温度计对应的测量值。几周后，你得到了一些数据:

``` python
t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c)
t_u = torch.tensor(t_u)
```

`t_c`是摄氏度数，`t_u`是未知单位度数。你可以假设两种测量结果中的噪声均来自温度计本身以及读数误差。为了方便起见，我们将数据转换成张量，你将很快使用它。

## 4.1.2 选择线性模型作为首次尝试

在没有进一步知识的情况下，我们先假定一个用于在两组测量之间相互转换的最简单的可能模型，就像开普勒所做的那样。两组数据可能是线性相关的，也就是说，将`t_u`乘以一个因子并加上一个常数就得到了摄氏温度：
$$
t_c = w * t_u + b
$$

这个假设合理吗？很可能是合理的，你最后会看到最终模型的效果如何。（$w$ 和 $b$ 分别是权重weight和偏差bias，这是线性缩放的两个常用术语，你将经常遇到。）

> 注意：我们知道线性模型是正确的，因为问题和数据都是我们捏造的，但请原谅我们；因为该模型是一个有用的激励示例（motivating example），有助于你了解PyTorch的内部工作机制。

现在，你需要根据已有的数据估算模型中的参数 $w$ 和 $b$。 为了根据未知温度 $t_u$ 获得以摄氏度为单位的温度值 $t_c$，你必须对这两个参数进行估计。该过程听起来像是通过一组测量值来拟合一条直线，那正是你正在做的事情。当你使用PyTorch实现此简单示例时，应意识到训练神经网络本质上就是通过调整一些（可能很大量的）参数将模型更改为更为精确的模型。

为了再次具体化示例，设想你有一个带有一些未知参数的模型，并且需要估计这些参数，以使预测输出与测量值之间的误差尽可能小。你注意到仍然需定义如何度量这种误差。如果误差很大，那么这种度量（我们称为损失函数）应该很高，而完美匹配时的理想情况下应该尽可能低。因此，你的优化过程应以找到使损失函数处于最低水平的 $w$ 和 $b$ 为目标。

## 4.1.3 你所需的是减少损失

损失函数（或成本函数）是输出为单个数值的函数，在学习过程中我们试图最小化它。损失函数通常是计算训练样本的期望输出与模型接收这些样本所产生的实际输出之间的差异，在本例中，即是模型输出的预测温度 $t_p$ 与实际测量值之间的差异 $ t_p-t_c$。

你需要确保在 $t_p$ 高于和低于真实 $t_c$ 时，损失函数都会输出正值，因为目标是使损失函数最小。（将损失优化至负无穷是没有用的。）你有几种选择，最直接的是 $|t_p-t_c|$ 和 $ (t_p-t_c)^2 $。基于你所选择的数学表达式，你可以强调或消除某些损失。从概念上讲，损失函数是一种从训练样本中优先确定的要修复哪些错误的方法，这样，参数更新将导致对高权重样本的输出进行调整，而不是对损失较小的其他样本输出进行更改。

刚刚的两个示例损失函数都具有明显的最小零值，并且随着预测值在任一方向上远离真实值而单调递增。由于这个原因，两个函数都被认为是凸的（convex）。因为你的模型是线性的，所以得到的关于 $w$ 和 $b$ 的损失函数也是凸的。损失函数是模型参数的凸函数的情况通常比较好处理，因为你可以通过专门的算法以有效的方式找到最小值。深度神经网络的损失函数不保证是凸的，因此这些方法通常对你没有用。

对于损失函数 $| t_p-t_c |$ 和 $(t_p-t_c)^2$，如图4.3所示，需要注意的是后者在最小值附近表现得更好：当 $t_p$ 等于 $t_c$ 时，误差平方损失相对于 $t_p$ 的导数为零。相反，绝对误差损失函数在你想要收敛的位置具有不确定的导数。实际上这个问题并没有看起来那么重要，但是暂时坚持使用误差平方损失。

值得注意的是，误差平方损失还比绝对误差损失更严重地惩罚了错误的结果。通常，稍微出错的结果是要好于一些严重错误的结果的，误差平方损失有助于按需要对这些结果进行优先级排序。

<div align=center>
<img width="500" src="../img/chapter4/4.3.png" alt="4.3"/>
</div>
<div align=center>图4.3 绝对误差与平方误差</div>